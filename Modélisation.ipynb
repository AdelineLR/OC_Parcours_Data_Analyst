{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e842e6",
   "metadata": {},
   "source": [
    "\n",
    "# Modélisation\n",
    "\n",
    "*OPENCLASSROOMS - Parcours Data Analyst V2 - Adeline Le Ray*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c8e68",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "* [3. Régressions]\n",
    "    * [3.1 Régression linéaire simples]\n",
    "        * [3.1.1. Méthode des moindres carrés ordinaires (MCO)]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778dbbdf",
   "metadata": {},
   "source": [
    "# 1. Echantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2cd8fe",
   "metadata": {},
   "source": [
    "## 1.1 Echantillonnage stratifié\n",
    "\n",
    "### Avec Pandas\n",
    "\n",
    "````\n",
    "# Création de l'échantillon d'entraînement et de l'échantillon de test en respectant la répartition globale\n",
    "train_2 = df_reg2.groupby('age').apply(lambda x: x.sample(frac=0.80)).droplevel(0)\n",
    "index=train_2.reset_index().iloc[:,0].tolist()\n",
    "test_2 = df_reg2.loc[~df_reg2.index.isin(index)]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b890c",
   "metadata": {},
   "source": [
    "# 3. Régressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4192d0",
   "metadata": {},
   "source": [
    "## 3.1 Régression linéaire \n",
    "\n",
    "### 3.1.1. Méthode des moindres carrés ordinaires (MCO)\n",
    "\n",
    "$\\hat{y_{i}}=\\hat{a}.x_{i} +\\hat{b}$ avec $\\hat{a}=\\frac{s_{X,Y}}{s_{X}^2}$ et $\\hat{b}=\\bar{y}-\\hat{a}.\\bar{x}$\n",
    "\n",
    "Le traitement statistique, la régression linéaire avec estimation par la méthode des moindres carrés, est peu robuste aux outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eda98c",
   "metadata": {},
   "source": [
    "###### Code :\n",
    "\n",
    "- **Calculs des dépenses et de l'attente**\n",
    "\n",
    "````\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# Selection du sous-échantillon\n",
    "courses = operations[operations.categ == \"COURSES\"]\n",
    "\n",
    "# On trie les opérations par date\n",
    "courses = courses.sort_values(\"date_operation\")\n",
    "\n",
    "# On ramène les montants en positif\n",
    "courses[\"montant\"] = -courses[\"montant\"]\n",
    "\n",
    "# calcul de la variable attente\n",
    "r = []\n",
    "last_date = dt.datetime.now()\n",
    "for i,row in courses.iterrows(): #Iterate over DataFrame rows as (index, Series) pairs.\n",
    "    days = (row[\"date_operation\"]-last_date).days\n",
    "    if days == 0:\n",
    "        r.append(r[-1])\n",
    "    else:\n",
    "        r.append(days)\n",
    "    last_date = row[\"date_operation\"]\n",
    "courses[\"attente\"] = r\n",
    "courses = courses.iloc[1:,]#df sans la première ligne\n",
    "\n",
    "# on regroupe les opérations qui ont été effectués à la même date\n",
    "# (courses réalisées le même jour mais dans 2 magasins différents)\n",
    "a = courses.groupby(\"date_operation\")[\"montant\"].sum()\n",
    "b = courses.groupby(\"date_operation\")[\"attente\"].first()\n",
    "courses = pd.DataFrame({\"montant\":a, \"attente\":b})#colonne 'montant'=a et attente = b\n",
    "````\n",
    "\n",
    "- **Calcul de $\\hat{a}$ et $\\hat{b}$**\n",
    "\n",
    "````\n",
    "import statsmodels.api as sm\n",
    "Y = courses['montant']\n",
    "X = courses[['attente']]\n",
    "X = X.copy() # On modifiera X, on en crée donc une copie\n",
    "X['intercept'] = 1.\n",
    "result = sm.OLS(Y, X).fit() # OLS = Ordinary Least Square (Moindres Carrés Ordinaire)\n",
    "a,b = result.params['attente'],result.params['intercept']\n",
    "````\n",
    "\n",
    "Une régression linéaire prédit une variable en fonction d'une ou plusieurs variables.  `sm.OLS`  s'attend donc à trouver une unique colonne (c.-à-d. un  `pd.Series`  ) en premier argument (ici `Y`), mais s'attend à trouver potentiellement plusieurs colonnes en 2nd argument (ici `X`, qui est un  `pd.DataFrame`  ). Pour sélectionner plusieurs colonnes d'un dataframe, on passe une liste de noms de colonnes. Et comme une liste s'écrit entre crochets, ceux-ci viennent s'ajouter aux crochets déjà présents !\n",
    "\n",
    "\n",
    "- **Affichage de la régression**\n",
    "````\n",
    "plt.plot(courses.attente,courses.montant, \"o\")\n",
    "plt.plot(np.arange(15),[a*x+b for x in np.arange(15)])\n",
    "plt.xlabel(\"attente\")\n",
    "plt.ylabel(\"montant\")\n",
    "plt.show()\n",
    "````\n",
    "\n",
    "En ligne 2,  np.arange  crée une liste de nombres entiers allant de 0 à 14 :  `[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]`  .\n",
    "\n",
    "On place cette liste en abscisse. Pour chacune de ces 15 valeurs, on calcule les ordonnées grâce à la formule y=ax+b comme ceci :  `[a*x+b for x in np.arange(15)]`  . On vient donc de créer une série de points tous alignés sur la droite d'équation y=ax+b. La ligne 2 affiche tous ces points, en les reliant entre eux, ce qui nous donne une belle ligne !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85625bea",
   "metadata": {},
   "source": [
    "### 3.1.2 Coefficient de détermination R²: pourcentage expliqué de la variance\n",
    "\n",
    "$$SCT=SCE+SCR$$ \n",
    "\n",
    "$$\\sum\\limits _{j=1} ^{n}(y_{j}-\\bar y)^2=\\sum\\limits _{j=1} ^{n}(\\hat{y_{j}}-\\bar y)^2 + \\sum\\limits _{j=1} ^{n}(y_{j}-\\hat y_{j})^2$$\n",
    "$SCT$ (somme des carrés totale) traduit la variation totale de Y , $SCE$ (somme des carrés expliquée) traduit la variation expliquée par le modèle et $SCR$ (somme des carrés résiduelle) traduit la variation inexpliquée par le modèle.\n",
    "\n",
    "Pour la régression linéaire, le pourcentage de variation expliquée est donné par le coefficient de détermination noté $R^2$ :\n",
    "\n",
    "$$R^2=\\frac{SCE}{SCT}$$\n",
    "\n",
    "####  avec statsmodels.api\n",
    "```\n",
    "import statsmodels.api as sm\n",
    "Y = dt['Position']\n",
    "X = dt[[\"Age\"]].copy()\n",
    "X['intercept'] = 1\n",
    "result = sm.OLS(Y, X).fit()\n",
    "\n",
    "a,b = result.params\n",
    "print(a, b, result.rsquared)\n",
    "\n",
    "```\n",
    "####  avec sklearn.metrics\n",
    "````\n",
    "# Calcul du coefficient de détermination R²\n",
    "from sklearn.metrics import r2_score \n",
    "R_square = r2_score(test_1.montant_total_achat, test_1.estim) \n",
    "print(\"Echantillon de test : Le coefficient de détermination R² de la régression linéaire de\", round(R_square,2))\n",
    "````\n",
    "\n",
    "\n",
    "#### Interprétation et utilisation\n",
    "\n",
    "On peut voir le $R^2$  comme l’erreur du modèle divisé par l’erreur d’un modèle basique qui prédit tout le temps la moyenne de la variable à prédire.\n",
    "- $R^2$ = 1 : modèle parfait\n",
    "- 0 < $R^2$ <1 : Le score $R^2$  est d’autant plus élevé que le modèle est performant, et vaut au maximum 100%, lorsque toutes les prédictions sont exactes. \n",
    "- $R^2$  <0 : Il n’y a pas de score minimum, mais un modèle simple prédisant tout le temps la valeur moyenne atteint un score R2 de 0%. Par conséquent un score R2 négatif signifie que les prédictions sont moins bonnes que si l’on prédisait systématiquement la valeur moyenne.\n",
    "\n",
    "\n",
    "Le $R^2$  a deux spécificités : \n",
    "- Il facilite la comparaison entre différents modèles. Dire qu’un modèle a une MSE de 25 ne permet pas de conclure si le modèle est correct car cela dépend des valeurs prises par la variable à prédire. Alors que la normalisation faite dans le $R^2$  permet de dire qu’un modèle ayant moins de 20% de $R^2$  n’est pas performant et qu’au contraire un modèle qui atteint plus de 80% de $R^2$  est performant.\n",
    "- Il est en revanche peu interprétable et ne donne pas d’information sur l’erreur moyenne du modèle. En effet, si le $R^2$  permet de comparer la performance du modèle avec une performance basique, il ne permet pas pour autant de dire quelle erreur est faite en moyenne sur les prédictions. Il faut souvent le combiner avec d’autres métriques afin de mieux comprendre la performance du modèle comme la MSE ou la MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014dd4d",
   "metadata": {},
   "source": [
    "````\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = df_complet.loc[(~df_complet['pop_ss_moyen_alim_saine_(%)'].isna())\\\n",
    "                   &(df_complet['c_income']!=9010),:].iloc[:,10:26]\n",
    "y = df_complet.loc[(~df_complet['pop_ss_moyen_alim_saine_(%)'].isna())\\\n",
    "                   &(df_complet['c_income']!=9010),'pop_ss_moyen_alim_saine_(%)'].values\n",
    "\n",
    "# Création de l'échantillon d'entraînement et de l'échantillon de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train,y_train) \n",
    "\n",
    "y_train_predict = model.predict(X_train) \n",
    "\n",
    "r_squared = r2_score(y_train,y_train_predict)\n",
    "print(r_squared)\n",
    "\n",
    "\n",
    "y_test_predict = model.predict(X_test) \n",
    "\n",
    "r_squared = r2_score(y_test,y_test_predict)\n",
    "print(r_squared)\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
